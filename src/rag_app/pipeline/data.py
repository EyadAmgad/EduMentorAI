documents = [
    "Deep learning is a subset of machine learning based on neural networks.",
    "A neural network consists of layers of interconnected nodes called neurons.",
    "Activation functions introduce non-linearity into neural networks.",
    "The most common activation functions are ReLU, sigmoid, and tanh.",
    "A convolutional neural network (CNN) is effective for image recognition tasks.",
    "Recurrent neural networks (RNNs) are designed for sequential data such as text or speech.",
    "Long Short-Term Memory (LSTM) networks solve the vanishing gradient problem in RNNs.",
    "Transformers use self-attention mechanisms to handle long-range dependencies in sequences.",
    "Dropout is a regularization technique to prevent overfitting.",
    "Batch normalization stabilizes and accelerates training by normalizing layer inputs.",
    "Gradient descent is the optimization algorithm used to minimize the loss function.",
    "Backpropagation is the method used to compute gradients in neural networks.",
    "The learning rate controls how much the model's weights are updated during training.",
    "Overfitting happens when a model learns noise instead of general patterns.",
    "Underfitting occurs when a model is too simple to capture data patterns.",
    "Transfer learning uses pre-trained models to improve performance on new tasks.",
    "Generative Adversarial Networks (GANs) consist of a generator and a discriminator.",
    "Autoencoders are used for unsupervised learning and dimensionality reduction.",
    "Attention mechanisms improve sequence-to-sequence models by focusing on relevant inputs.",
    "Reinforcement learning trains agents through rewards and penalties."
]
